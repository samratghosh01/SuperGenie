# BI Genie — Environment Configuration
# Copy this file to .env and fill in your values

# ── LLM Configuration ────────────────────────────────────────────────────────
# API key for your LLM provider (OpenAI, Anthropic, or LiteLLM proxy)
LITELLM_API_KEY=your-api-key-here

# Base URL for the LLM API (e.g., https://api.openai.com/v1 or your LiteLLM proxy)
LITELLM_URL=https://your-llm-endpoint.example.com

# Model name (examples: gpt-4o, claude-haiku-4-5@20251001, etc.)
LLM_MODEL=claude-haiku-4-5@20251001

# ── Superset Admin ────────────────────────────────────────────────────────────
SUPERSET_ADMIN_PASSWORD=change-me-to-strong-password
SUPERSET_SECRET_KEY=change-me-to-random-secret-key

# ── Database ──────────────────────────────────────────────────────────────────
POSTGRES_PASSWORD=change-me-to-strong-password

# ── URLs (update for production / EC2 deployment) ────────────────────────────
# External URL where the backend is accessible (used by Superset to load chat iframe)
BACKEND_URL=http://localhost:9000

# External URL where Superset is accessible (used in dashboard links returned to users)
SUPERSET_EXTERNAL_URL=http://localhost:9088

# ── Optional: TLS settings ───────────────────────────────────────────────────
# Set to "true" to skip TLS verification for the LLM endpoint (self-signed certs)
# LLM_SKIP_TLS_VERIFY=false
